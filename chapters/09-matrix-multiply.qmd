---
title: "Investigation: Matrix Multiply"
subtitle: "Why This Operation Rules Modern Computing"
id: sec-matrix-multiply
---

::: {.chapter-opener}
Two matrix multiply implementations. Same algorithm. Same hardware. Same inputs.

One takes 8 seconds. The other takes 40 milliseconds.

That's a **200× difference**. How?
:::

## The Setup

We have two 2048×2048 matrices of 32-bit floats. We want to compute their product.

```python
import numpy as np

M = K = N = 2048
A = np.random.randn(M, K).astype(np.float32)
B = np.random.randn(K, N).astype(np.float32)
C = np.zeros((M, N), dtype=np.float32)
```

Data sizes:
- A: 2048 × 2048 × 4 bytes = 16 MB
- B: 2048 × 2048 × 4 bytes = 16 MB
- C: 2048 × 2048 × 4 bytes = 16 MB
- Total: 48 MB

Operations required:
- Each of 2048² output elements requires 2048 multiply-adds
- Total: 2048³ × 2 = 17.2 billion FLOPs

## Implementation 1: The Naive Approach

```python
def matmul_naive(A, B, C):
    M, K = A.shape
    _, N = B.shape
    for i in range(M):
        for j in range(N):
            for k in range(K):
                C[i, j] += A[i, k] * B[k, j]
    return C
```

Let's trace what happens for each output element:

```
Computing C[0, 0]:
  for k in range(2048):
      C[0, 0] += A[0, k] * B[k, 0]

  A accesses: A[0,0], A[0,1], A[0,2], ... A[0,2047]
              → Sequential along row. Good!

  B accesses: B[0,0], B[1,0], B[2,0], ... B[2047,0]
              → Strided by 2048 elements (8 KB). Terrible!
```

The problem is **B's access pattern**. In row-major storage:

```
B in memory:
[B[0,0] B[0,1] B[0,2] ... B[0,2047]] [B[1,0] B[1,1] ... B[1,2047]] ...

Accessing B[:,0] means jumping 2048 elements (8 KB) between accesses.
Each access is a cache miss. We touch 2048 cache lines to read one column.
```

### Benchmark

```
Naive Python:     ~800 seconds (too slow, Numba version)
Naive Numba:      ~8 seconds
Expected:         2048³ × 2 / (10 GFLOPS) ≈ 1.7 seconds
```

We're getting only **2 GFLOPS**—far below even single-core theoretical (~15 GFLOPS for modern CPUs).

## Implementation 2: Loop Reordering

Before tiling, let's try a simpler fix: reorder the loops.

```python
def matmul_reordered(A, B, C):
    M, K = A.shape
    _, N = B.shape
    for i in range(M):
        for k in range(K):
            for j in range(N):
                C[i, j] += A[i, k] * B[k, j]
    return C
```

Wait—we just swapped the `j` and `k` loops. Same computation, right?

Mathematically, yes. For hardware, completely different:

```
Computing partial results for row 0:
  for k in range(2048):
      a_val = A[0, k]  # Scalar, stays in register
      for j in range(2048):
          C[0, j] += a_val * B[k, j]

  A accesses: A[0,0], A[0,1], ... (one per outer iteration)
              → Sequential. Good!

  B accesses: B[0,0], B[0,1], B[0,2], ... B[0,2047]
              then B[1,0], B[1,1], B[1,2], ... B[1,2047]
              → Sequential along rows. Good!

  C accesses: C[0,0], C[0,1], ... C[0,2047] (each outer iteration)
              → Sequential along row, repeated. Good!
```

### Benchmark

```
Reordered Numba:  ~2 seconds
Naive Numba:      ~8 seconds
Speedup:          4×
```

Just by reordering loops—no algorithmic change—we get 4× speedup.

But we're still at only **8 GFLOPS**. The hardware can do much more.

## Why We're Still Slow

Let's compute the **arithmetic intensity**:

```python
# Data movement
bytes_read = (M * K + K * N) * 4  # Read A and B once
bytes_written = M * N * 4         # Write C once
total_bytes = bytes_read + bytes_written
# = (16 + 16 + 16) MB = 48 MB

# Computation
flops = M * K * N * 2  # multiply-add = 2 ops
# = 2048³ × 2 = 17.2 billion FLOPs

# Arithmetic intensity
intensity = flops / total_bytes
# = 17.2e9 / 48e6 = 358 FLOPs/byte
```

Wait—358 FLOPs/byte is *extremely* high. We should be compute-bound.

But that calculation assumes we read each input element only once. In our naive loop:

```
A[i, k] is read once for each j.
  Total A reads: M × K × N = 2048³ = 8.6 billion
  Unique A elements: M × K = 4 million

B[k, j] is read once for each i.
  Total B reads: M × K × N = 2048³ = 8.6 billion
  Unique B elements: K × N = 4 million

Reuse factor: 2048× for each element
```

We're reading the same data **2048 times** from memory because it doesn't fit in cache.

The *effective* arithmetic intensity:

```
Effective bytes moved ≈ 2 × M × K × N × 4 = 68 GB
Effective intensity = 17.2e9 / 68e9 = 0.25 FLOPs/byte
```

At 0.25 FLOPs/byte, we're **memory-bound**. The roofline model says:

```
Peak memory-bound performance = bandwidth × intensity
                              = 50 GB/s × 0.25 FLOPs/byte
                              = 12.5 GFLOPS
```

Our 8 GFLOPS is close to this limit. We can't go faster without improving data reuse.

## Implementation 3: Tiling

The solution: **process in cache-sized blocks**.

```python
def matmul_tiled(A, B, C, tile_size=64):
    M, K = A.shape
    _, N = B.shape

    for i0 in range(0, M, tile_size):
        for j0 in range(0, N, tile_size):
            for k0 in range(0, K, tile_size):
                # Load tiles into (implicit) cache
                i1 = min(i0 + tile_size, M)
                j1 = min(j0 + tile_size, N)
                k1 = min(k0 + tile_size, K)

                # Compute tile contribution to C
                for i in range(i0, i1):
                    for k in range(k0, k1):
                        a_val = A[i, k]
                        for j in range(j0, j1):
                            C[i, j] += a_val * B[k, j]

    return C
```

Why does this work?

```
Tile size = 64
Each tile: 64 × 64 × 4 bytes = 16 KB

Per outer iteration:
  - A tile: 16 KB
  - B tile: 16 KB
  - C tile: 16 KB (read-modify-write)
  - Total: 48 KB

L1 cache: 32-64 KB
L2 cache: 256-512 KB

Working set fits in L2, often in L1!
```

Within each tile, we access the same 16 KB of A, B, C for 64³ = 262,144 operations.

### Effective Reuse Analysis

Without tiling:
```
A[i,k] read M×N times, stored once → reuse = 1
B[k,j] read M×N times, stored once → reuse = 1
```

With tiling (tile size T):
```
A[i,k] read (N/T) times, stored once → reuse = N/T = 32×
B[k,j] read (M/T) times, stored once → reuse = M/T = 32×
```

Effective data movement drops by ~32×. We're no longer memory-bound.

::: {.callout-tip collapse="true"}
## Interactive: Tiled Matrix Multiply Visualizer

Explore how tiling enables data reuse. Adjust matrix and tile sizes to see how working set and reuse factor change.

```{ojs}
//| echo: false

viewof matrixSize = Inputs.range([256, 4096], {
  value: 2048,
  step: 256,
  label: "Matrix Size (N×N)"
})

viewof tileSizeMatmul = Inputs.range([16, 256], {
  value: 64,
  step: 16,
  label: "Tile Size (T×T)"
})

viewof cacheSize = Inputs.select([32, 64, 256, 512, 1024], {
  value: 256,
  label: "Cache Size (KB)"
})
```

```{ojs}
//| echo: false

// Memory calculations
elemSize = 4  // float32

// Naive approach - no tiling
naiveBytesPerOutput = 2 * matrixSize * elemSize  // Read full row of A and column of B
naiveTotalReads = matrixSize * matrixSize * matrixSize * 2 * elemSize  // Each element read N times
naiveReuse = 1

// Tiled approach
tileBytes = tileSizeMatmul * tileSizeMatmul * elemSize
workingSet = 3 * tileBytes  // A tile + B tile + C tile
workingSetKB = workingSet / 1024

// How many times each element is reused
tiledReuse = matrixSize / tileSizeMatmul
numTiles = Math.pow(matrixSize / tileSizeMatmul, 3)

// Effective data movement
tiledTotalReads = 2 * matrixSize * matrixSize * (matrixSize / tileSizeMatmul) * elemSize
reductionFactor = naiveTotalReads / tiledTotalReads

// Does it fit in cache?
fitsInCache = workingSetKB <= cacheSize

// Arithmetic intensity
flops = 2 * Math.pow(matrixSize, 3)
naiveIntensity = flops / naiveTotalReads
tiledIntensity = flops / tiledTotalReads
```

```{ojs}
//| echo: false

html`<div style="margin: 20px 0;">
  <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">

    <div style="padding: 20px; background: linear-gradient(135deg, #fee2e2, #fecaca); border-radius: 12px; border-left: 4px solid #ef4444;">
      <h4 style="margin: 0 0 15px 0; color: #b91c1c;">Naive (No Tiling)</h4>
      <div style="font-size: 0.9em; color: #7f1d1d;">
        <div style="margin-bottom: 8px;">
          <strong>Working Set:</strong> Full matrix (~${(matrixSize * matrixSize * elemSize / 1e6).toFixed(0)} MB)
        </div>
        <div style="margin-bottom: 8px;">
          <strong>Data Reuse:</strong> ${naiveReuse}× (none)
        </div>
        <div style="margin-bottom: 8px;">
          <strong>Memory Reads:</strong> ${(naiveTotalReads / 1e9).toFixed(1)} GB
        </div>
        <div>
          <strong>Arithmetic Intensity:</strong> ${naiveIntensity.toFixed(2)} FLOPs/byte
        </div>
      </div>
      <div style="margin-top: 12px; padding: 8px; background: #fecaca; border-radius: 6px; font-size: 0.8em; color: #991b1b;">
        Memory-bound: Each element re-read ${matrixSize}×
      </div>
    </div>

    <div style="padding: 20px; background: linear-gradient(135deg, #dcfce7, #bbf7d0); border-radius: 12px; border-left: 4px solid #22c55e;">
      <h4 style="margin: 0 0 15px 0; color: #15803d;">Tiled (T=${tileSizeMatmul})</h4>
      <div style="font-size: 0.9em; color: #14532d;">
        <div style="margin-bottom: 8px;">
          <strong>Working Set:</strong> ${workingSetKB.toFixed(1)} KB
          ${fitsInCache ?
            html`<span style="color: #16a34a; font-weight: bold;"> ✓ Fits in ${cacheSize}KB cache!</span>` :
            html`<span style="color: #dc2626; font-weight: bold;"> ✗ Exceeds ${cacheSize}KB cache</span>`}
        </div>
        <div style="margin-bottom: 8px;">
          <strong>Data Reuse:</strong> ${tiledReuse.toFixed(0)}× per tile load
        </div>
        <div style="margin-bottom: 8px;">
          <strong>Memory Reads:</strong> ${(tiledTotalReads / 1e9).toFixed(1)} GB
        </div>
        <div>
          <strong>Arithmetic Intensity:</strong> ${tiledIntensity.toFixed(1)} FLOPs/byte
        </div>
      </div>
      <div style="margin-top: 12px; padding: 8px; background: #bbf7d0; border-radius: 6px; font-size: 0.8em; color: #166534;">
        ${reductionFactor.toFixed(0)}× less memory traffic
      </div>
    </div>
  </div>

  <div style="margin-top: 20px; padding: 20px; background: linear-gradient(135deg, #f0f9ff, #e0f2fe); border-radius: 12px;">
    <h5 style="margin: 0 0 15px 0; color: #0369a1;">Tiling Visualization</h5>
    <div style="display: flex; gap: 40px; align-items: flex-start;">

      <div style="text-align: center;">
        <div style="font-size: 0.8em; color: #666; margin-bottom: 8px;">Matrix A (${matrixSize}×${matrixSize})</div>
        <div style="position: relative; width: 120px; height: 120px; background: #e0f2fe; border: 2px solid #0284c7; border-radius: 4px;">
          <div style="position: absolute; top: 0; left: 0; width: ${120 * tileSizeMatmul / matrixSize}px; height: ${120 * tileSizeMatmul / matrixSize}px; background: #0284c7; opacity: 0.7; border-radius: 2px;"></div>
          <div style="position: absolute; bottom: 5px; right: 5px; font-size: 0.7em; color: #0369a1;">A tile</div>
        </div>
      </div>

      <div style="font-size: 2em; color: #666; margin-top: 50px;">×</div>

      <div style="text-align: center;">
        <div style="font-size: 0.8em; color: #666; margin-bottom: 8px;">Matrix B (${matrixSize}×${matrixSize})</div>
        <div style="position: relative; width: 120px; height: 120px; background: #fef3c7; border: 2px solid #d97706; border-radius: 4px;">
          <div style="position: absolute; top: 0; left: 0; width: ${120 * tileSizeMatmul / matrixSize}px; height: ${120 * tileSizeMatmul / matrixSize}px; background: #d97706; opacity: 0.7; border-radius: 2px;"></div>
          <div style="position: absolute; bottom: 5px; right: 5px; font-size: 0.7em; color: #92400e;">B tile</div>
        </div>
      </div>

      <div style="font-size: 2em; color: #666; margin-top: 50px;">=</div>

      <div style="text-align: center;">
        <div style="font-size: 0.8em; color: #666; margin-bottom: 8px;">Matrix C (${matrixSize}×${matrixSize})</div>
        <div style="position: relative; width: 120px; height: 120px; background: #fce7f3; border: 2px solid #db2777; border-radius: 4px;">
          <div style="position: absolute; top: 0; left: 0; width: ${120 * tileSizeMatmul / matrixSize}px; height: ${120 * tileSizeMatmul / matrixSize}px; background: #db2777; opacity: 0.7; border-radius: 2px;"></div>
          <div style="position: absolute; bottom: 5px; right: 5px; font-size: 0.7em; color: #9d174d;">C tile</div>
        </div>
      </div>

      <div style="margin-left: 20px; font-size: 0.85em; color: #374151; max-width: 200px;">
        <div style="margin-bottom: 8px;"><strong>${numTiles.toFixed(0)} tile operations</strong></div>
        <div style="margin-bottom: 4px;">Each A tile: ${(tileBytes / 1024).toFixed(1)} KB</div>
        <div style="margin-bottom: 4px;">Each B tile: ${(tileBytes / 1024).toFixed(1)} KB</div>
        <div>Each C tile: ${(tileBytes / 1024).toFixed(1)} KB</div>
      </div>
    </div>
  </div>

  <div style="margin-top: 15px; padding: 15px; background: #f9fafb; border-radius: 8px; font-size: 0.85em; color: #4b5563;">
    <strong>Key insight:</strong> Each tile of A and B is loaded once but used for T² multiply-adds (reuse = ${tileSizeMatmul}). Smaller tiles have less reuse but fit in faster cache levels. The optimal tile size balances these tradeoffs.
  </div>
</div>`
```
:::

### Benchmark

```
Tiled Numba (T=64):  ~400 ms
Reordered Numba:     ~2 seconds
Speedup:             5×
```

Now at **43 GFLOPS**—approaching single-core limits. But BLAS does even better.

## Implementation 4: What BLAS Does

Libraries like OpenBLAS, MKL, and cuBLAS achieve near-peak performance. Here's how:

### 1. Multi-level Tiling

BLAS uses multiple tile sizes for multiple cache levels:

```
L1 tile:  32 × 32 (fits in L1)
L2 tile:  128 × 128 (fits in L2)
L3 tile:  512 × 512 (fits in L3)

Data flows: L3 → L2 → L1 → registers
Each level maximizes reuse before accessing slower memory.
```

### 2. Register Tiling

The innermost kernel keeps data in CPU registers:

```
Inner kernel: 8 × 8 micro-tile
  - 8 values from A in registers
  - 8 values from B in registers
  - 64 accumulators for C in registers

  All 64 multiply-adds use register operands.
  No memory access in inner loop.
```

### 3. SIMD Vectorization

Modern CPUs process multiple floats per instruction:

```
AVX-512:  16 floats per instruction
AVX2:     8 floats per instruction
SSE:      4 floats per instruction

Peak: 32+ FLOPs per cycle instead of 2
```

### 4. Cache-Friendly Packing

Before the main loop, BLAS *repacks* matrices into contiguous blocks:

```python
def pack_A(A, tile_size):
    """Rearrange A so each tile is contiguous"""
    M, K = A.shape
    packed = []
    for i0 in range(0, M, tile_size):
        for k0 in range(0, K, tile_size):
            tile = A[i0:i0+tile_size, k0:k0+tile_size]
            packed.append(tile.ravel())  # Flatten tile
    return np.concatenate(packed)
```

This ensures:
- Sequential memory access within tiles
- No cache conflicts between tiles
- Perfect prefetching

:::: {.callout-note}
## Zero-copy transpose: row vs column major

BLAS/cuBLAS expects **column-major** layout. Most C/C++/Python arrays are
**row-major**. The key identity is that a row-major matrix has the **same byte
layout** as its transpose in column-major. For $C = AB$:

$$A_{\text{row}} \equiv (A^T)_{\text{col}}, \quad B_{\text{row}} \equiv (B^T)_{\text{col}}$$
$$C^T = B^T A^T$$

So you can **swap operands and flip transpose flags** to get the same result,
with no explicit transpose or data movement. This is why frameworks "just work"
with cuBLAS: they pass leading dimensions and op flags, not extra copy kernels.

**Caveat**: This is free only for contiguous (or simple stride) layouts. For
non-contiguous views, you may need a real copy or a different kernel.
::::

### 5. Prefetching

BLAS explicitly prefetches the next tile while computing the current one:

```
Time: |--compute tile 0--|--compute tile 1--|--compute tile 2--|
Data: |--load tile 0-----|--load tile 1-----|--load tile 2-----|
                     ↑                  ↑
                     Prefetch tile 1    Prefetch tile 2
```

Memory latency is hidden by overlapping with computation.

### BLAS Benchmark

```
NumPy (calls BLAS):   ~40 ms
Our best tiled:       ~400 ms
Speedup:              10×

BLAS GFLOPS:          430 GFLOPS (multi-core)
Our GFLOPS:           43 GFLOPS (single-core)
```

## GPU: A Different Beast

GPUs change the game entirely. Let's see why.

### GPU Architecture Recap

```
GPU (e.g., A100):
  - 108 Streaming Multiprocessors (SMs)
  - Each SM: 64 CUDA cores
  - Total: 6912 cores
  - Memory bandwidth: 2 TB/s
  - Peak FP32: 19.5 TFLOPS
  - SRAM per SM: 192 KB
  - HBM (main memory): 80 GB
```

The key insight: GPUs are **massively parallel** but have the same memory hierarchy challenge.

### Naive GPU Matmul

```python
# One thread per output element
@cuda.jit
def matmul_naive_gpu(A, B, C):
    i, j = cuda.grid(2)
    if i < C.shape[0] and j < C.shape[1]:
        acc = 0.0
        for k in range(A.shape[1]):
            acc += A[i, k] * B[k, j]
        C[i, j] = acc
```

This is slow because:
1. Each thread independently loads A[i,:] and B[:,j] from global memory
2. No data sharing between threads
3. Memory bandwidth saturated, compute idle

### Tiled GPU Matmul (Shared Memory)

```python
TILE_SIZE = 32

@cuda.jit
def matmul_tiled_gpu(A, B, C):
    # Shared memory for tiles
    sA = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=float32)
    sB = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=float32)

    tx = cuda.threadIdx.x
    ty = cuda.threadIdx.y
    bx = cuda.blockIdx.x
    by = cuda.blockIdx.y

    row = by * TILE_SIZE + ty
    col = bx * TILE_SIZE + tx

    acc = 0.0

    for k0 in range(0, A.shape[1], TILE_SIZE):
        # Cooperatively load tiles into shared memory
        if row < A.shape[0] and k0 + tx < A.shape[1]:
            sA[ty, tx] = A[row, k0 + tx]
        if k0 + ty < B.shape[0] and col < B.shape[1]:
            sB[ty, tx] = B[k0 + ty, col]

        cuda.syncthreads()

        # Compute partial result using shared memory
        for k in range(TILE_SIZE):
            acc += sA[ty, k] * sB[k, tx]

        cuda.syncthreads()

    if row < C.shape[0] and col < C.shape[1]:
        C[row, col] = acc
```

What's happening:
1. Each thread block loads a tile of A and B into shared memory (fast SRAM)
2. All 32×32 = 1024 threads in the block share these tiles
3. Each thread computes one output element using the shared tiles
4. Tiles are loaded once, used 32× (by each thread in a row/column)

### Tensor Cores

Modern GPUs have dedicated matrix multiply units:

```
Tensor Core operation:
  D = A × B + C
  where A, B are 16×16 matrices (FP16)
        C, D are 16×16 matrices (FP32)

  One Tensor Core: 256 FLOPs per cycle
  A100 has 432 Tensor Cores
  Peak (theoretical, A100 SXM): 312 TFLOPS (FP16) or 156 TFLOPS (TF32)
```

Tensor Cores are purpose-built for matrix multiply. They achieve 10-20× higher throughput than CUDA cores.

### GPU Benchmark

```
Size: 2048×2048

Naive GPU:              ~50 ms
Tiled GPU (shared mem): ~5 ms
cuBLAS (Tensor Cores):  ~0.5 ms

CPU (single-core):      ~400 ms (our best)
CPU (NumPy/BLAS):       ~40 ms
GPU (cuBLAS):           ~0.5 ms

GPU vs CPU speedup:     80×
```

## Why Neural Networks Love GEMM

Matrix multiplication (GEMM = General Matrix Multiply) underlies almost every neural network operation:

### Fully Connected Layers

```python
# y = Wx + b
def linear(x, W, b):
    return x @ W.T + b

# For batch_size=32, input=768, output=3072:
# x: [32, 768], W: [3072, 768]
# This is matmul: [32, 768] × [768, 3072] = [32, 3072]
```

### Attention

```python
# Q, K, V projections
Q = input @ W_Q  # [seq, dim] × [dim, dim]
K = input @ W_K
V = input @ W_V

# Attention scores
scores = Q @ K.T  # [seq, dim] × [dim, seq] = [seq, seq]

# Output
output = softmax(scores) @ V  # [seq, seq] × [seq, dim]
```

Five matrix multiplications per attention layer.

### Convolutions

```python
# im2col transformation converts convolution to matmul
# Input: [batch, channels, H, W]
# Kernel: [out_channels, in_channels, kH, kW]

# im2col extracts patches: [batch * H_out * W_out, in_channels * kH * kW]
# Kernel reshaped: [out_channels, in_channels * kH * kW]

# Convolution becomes matmul:
# [batch * H_out * W_out, in_channels * kH * kW] × [in_channels * kH * kW, out_channels]
```

This is why GPUs (and TPUs) are optimized for matrix multiply. A 10× improvement in GEMM is a 10× improvement in neural network training.

## The Properties at Work

Let's identify which properties make fast matrix multiply possible:

### Associativity

```
C[i,j] = Σ_k A[i,k] × B[k,j]

This sum is associative: we can compute in any order.
- Tile-by-tile: Σ over tiles, then Σ within tiles
- Parallel: Different threads compute different partial sums
```

Associativity enables **tiling and parallelism**.

### Locality

```
Tiling keeps working set small:
  - Process 64×64 tiles
  - 48 KB working set fits in L2
  - 2048× reuse before eviction
```

Locality enables **cache efficiency**.

### Separability

The fundamental identity:

```
(A × B)[i,j] = Σ_k A[i,k] × B[k,j]
            = A[i,:] · B[:,j]

Output is the *inner product* of row i of A with column j of B.
```

The computation separates by output element—each can be computed independently.

This enables **parallelism** (each thread computes different outputs).

### Hardware Co-Design

Tensor Cores are *built for* matrix multiply:

```
Input: Two 16×16 matrices
Output: One 16×16 matrix
Latency: A few cycles

This isn't general-purpose hardware accelerating matmul.
This is matmul-shaped hardware.
```

The algorithm (tiled GEMM) and the hardware (tensor cores, shared memory hierarchy) evolved together.

## Property Audit

| Property | Applies? | How it's exploited |
|----------|----------|-------------------|
| **Associativity** | **Yes** | Matmul can be decomposed into independent sub-problems (tiles) and recombined — the result is the same regardless of computation order |
| **Separability** | Partial | Each output element is an inner product (separable across the K dimension). Strassen-type algorithms exploit deeper factorizations |
| **Locality** | **Primary** | Tiling keeps working sets in cache/SRAM. The 200× speedup is almost entirely from improving data locality |
| **Sparsity** | No | Dense matmul — no zeros to skip |
| **Redundancy** | Partial | FP16/INT8 tensor cores exploit that full FP32 precision is often unnecessary |
| **Symmetry** | No | General matrices have no symmetry to exploit (symmetric/Hermitian matrices do) |

**Dominant property**: Locality — the entire optimization journey from naive to cuBLAS is about keeping data in fast memory.

## Key Takeaways

1. **200× speedup is possible**: From naive loops to optimized BLAS/cuBLAS, the same algorithm runs 200× faster. All from exploiting memory hierarchy.

2. **Memory access pattern dominates**: Loop reordering alone gives 4× speedup. The algorithm is the same; only the access pattern changes.

3. **Tiling transforms memory-bound to compute-bound**: By keeping working sets in cache, we achieve the theoretical arithmetic intensity.

4. **GPU shared memory is like CPU cache**: Tiling on GPU uses shared memory (fast SRAM) to enable data reuse across threads.

5. **Hardware is specialized**: Tensor Cores exist because matrix multiply is *that* important. Modern ML is built on GEMM.

6. **All four properties contribute**:
   - Associativity → parallelism and tiling
   - Locality → cache efficiency
   - Separability → independent computation per output
   - (Sparsity → structured sparse GEMM, Chapter 6)

## The Meta-Lesson

Matrix multiply is the *Drosophila* of performance engineering. It's simple enough to understand completely, yet rich enough to demonstrate every optimization technique.

When you see a 200× speedup, you're seeing:
- 4× from access patterns (loop order)
- 10× from tiling (cache reuse)
- 5× from vectorization (SIMD)
- 10× from Tensor Cores (specialized hardware)

These are multiplicative. Miss any one, and you leave 10× on the table.

The best algorithms aren't just correct. They're *aligned* with hardware.

::: {.callout-note}
## Try It Yourself

The accompanying notebook lets you:

- Implement and benchmark naive, reordered, and tiled matmul
- Visualize cache behavior with different tile sizes
- Compare CPU vs GPU performance
- Profile memory bandwidth vs compute utilization

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ttsugriy/performance-book/blob/main/notebooks/tier2-experimental/10-matrix-multiply.ipynb)
:::

## Further Reading

- Goto & Van De Geijn (2008). "Anatomy of High-Performance Matrix Multiplication"
- Low et al. (2016). "Analytical Modeling Is Enough for High-Performance BLIS"
- Jia et al. (2018). "Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking"
- NVIDIA CUTLASS: https://github.com/NVIDIA/cutlass


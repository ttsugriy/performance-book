---
title: "Investigation: LoRA"
subtitle: "The Low-Rank Structure of Fine-Tuning"
status: draft
---

::: {.callout-warning}
## Chapter In Progress
This chapter is currently being written. Check back soon!
:::

## Coming Soon

This investigation will:

- Start with the puzzle: how can 99.99% fewer parameters work?
- Measure the **intrinsic dimensionality** of fine-tuning
- Understand why **pretrained models have low-rank updates**
- See when the low-rank bet **fails**

---

[**← Previous: FlashAttention Investigation**](11-flash-attention.qmd) | [**Next: Quantization Investigation →**](13-quantization.qmd)

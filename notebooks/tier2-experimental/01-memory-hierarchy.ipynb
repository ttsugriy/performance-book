{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation: The Memory Hierarchy\n",
    "\n",
    "**From The Nature of Fast, Chapter 1**\n",
    "\n",
    "This notebook lets you investigate the memory hierarchy on your own hardware. You'll see how access patterns dramatically affect performance—even when the algorithm is \"the same.\"\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ttsugriy/performance-book/blob/main/notebooks/tier2-experimental/01-memory-hierarchy.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's check our environment and import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Processor: {platform.processor()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation 1: Sequential vs Random Access\n",
    "\n",
    "Let's measure how access pattern affects performance for summing an array.\n",
    "\n",
    "**Hypothesis (RAM model)**: Both should be the same—we're accessing the same elements.\n",
    "\n",
    "**Hypothesis (Memory hierarchy)**: Sequential should be much faster due to caching and prefetching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_access_patterns(n=100_000_000, num_runs=5):\n",
    "    \"\"\"Compare sequential vs random array access.\"\"\"\n",
    "    \n",
    "    # Create array and random indices\n",
    "    print(f\"Creating array of {n:,} elements ({n * 8 / 1e9:.2f} GB)...\")\n",
    "    arr = np.arange(n, dtype=np.int64)\n",
    "    indices = np.random.permutation(n).astype(np.int64)\n",
    "    \n",
    "    # Warmup\n",
    "    _ = arr.sum()\n",
    "    _ = arr[indices[:1000]].sum()\n",
    "    \n",
    "    # Benchmark sequential access\n",
    "    print(\"\\nBenchmarking sequential access...\")\n",
    "    seq_times = []\n",
    "    for i in range(num_runs):\n",
    "        start = time.perf_counter()\n",
    "        total_seq = arr.sum()\n",
    "        seq_times.append(time.perf_counter() - start)\n",
    "        print(f\"  Run {i+1}: {seq_times[-1]:.4f}s\")\n",
    "    \n",
    "    # Benchmark random access\n",
    "    print(\"\\nBenchmarking random access...\")\n",
    "    rand_times = []\n",
    "    for i in range(num_runs):\n",
    "        start = time.perf_counter()\n",
    "        total_rand = arr[indices].sum()\n",
    "        rand_times.append(time.perf_counter() - start)\n",
    "        print(f\"  Run {i+1}: {rand_times[-1]:.4f}s\")\n",
    "    \n",
    "    # Verify correctness\n",
    "    assert total_seq == total_rand, \"Results don't match!\"\n",
    "    \n",
    "    # Report results\n",
    "    seq_mean = np.mean(seq_times)\n",
    "    rand_mean = np.mean(rand_times)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Sequential: {seq_mean:.4f}s (±{np.std(seq_times):.4f}s)\")\n",
    "    print(f\"Random:     {rand_mean:.4f}s (±{np.std(rand_times):.4f}s)\")\n",
    "    print(f\"Ratio:      {rand_mean/seq_mean:.1f}× slower\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return seq_times, rand_times\n",
    "\n",
    "# Run the benchmark\n",
    "seq_times, rand_times = benchmark_access_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "You should see random access being 5-20× slower than sequential.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "1. **Cache lines**: When you access one element, the CPU fetches a whole cache line (64 bytes = 8 int64s). Sequential access uses all 8; random access wastes 7.\n",
    "\n",
    "2. **Prefetching**: The CPU hardware predicts sequential patterns and fetches ahead. Random access defeats prediction.\n",
    "\n",
    "3. **TLB misses**: Virtual-to-physical address translation is cached. Random access causes more TLB misses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation 2: Working Set Size\n",
    "\n",
    "How does array size affect performance? If the working set fits in cache, access is fast. If it spills to DRAM, access slows down.\n",
    "\n",
    "Let's find the cache boundaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_working_set_sizes():\n",
    "    \"\"\"Measure how working set size affects access speed.\"\"\"\n",
    "    \n",
    "    # Range of sizes from 1 KB to 1 GB\n",
    "    sizes_kb = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, \n",
    "                1024, 2048, 4096, 8192, 16384, 32768, \n",
    "                65536, 131072, 262144, 524288, 1048576]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for size_kb in sizes_kb:\n",
    "        # Create array\n",
    "        n_elements = (size_kb * 1024) // 8  # 8 bytes per int64\n",
    "        if n_elements < 1000:\n",
    "            continue\n",
    "            \n",
    "        arr = np.arange(n_elements, dtype=np.int64)\n",
    "        \n",
    "        # Measure random access (to stress the cache)\n",
    "        indices = np.random.randint(0, n_elements, size=min(n_elements, 10_000_000))\n",
    "        \n",
    "        # Warmup\n",
    "        _ = arr[indices[:1000]].sum()\n",
    "        \n",
    "        # Measure\n",
    "        times = []\n",
    "        for _ in range(3):\n",
    "            start = time.perf_counter()\n",
    "            _ = arr[indices].sum()\n",
    "            times.append(time.perf_counter() - start)\n",
    "        \n",
    "        mean_time = np.mean(times)\n",
    "        accesses = len(indices)\n",
    "        ns_per_access = (mean_time / accesses) * 1e9\n",
    "        \n",
    "        results.append({\n",
    "            'size_kb': size_kb,\n",
    "            'ns_per_access': ns_per_access,\n",
    "        })\n",
    "        \n",
    "        print(f\"Size: {size_kb:>7} KB | {ns_per_access:>6.1f} ns/access\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = benchmark_working_set_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "sizes = [r['size_kb'] for r in results]\n",
    "latencies = [r['ns_per_access'] for r in results]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.loglog(sizes, latencies, 'o-', linewidth=2, markersize=8)\n",
    "\n",
    "# Add cache size annotations (typical values)\n",
    "cache_sizes = [\n",
    "    (32, 'L1 (~32 KB)'),\n",
    "    (256, 'L2 (~256 KB)'),\n",
    "    (32768, 'L3 (~32 MB)'),\n",
    "]\n",
    "\n",
    "for size, label in cache_sizes:\n",
    "    plt.axvline(x=size, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.text(size * 1.1, max(latencies) * 0.8, label, fontsize=10, rotation=90)\n",
    "\n",
    "plt.xlabel('Working Set Size (KB)', fontsize=12)\n",
    "plt.ylabel('Latency per Access (ns)', fontsize=12)\n",
    "plt.title('Memory Latency vs Working Set Size', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: The 'steps' in the curve show cache boundaries.\")\n",
    "print(\"When data spills from one cache level to the next, latency increases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Look For\n",
    "\n",
    "You should see a **staircase pattern**:\n",
    "\n",
    "1. **Flat region (small sizes)**: Data fits in L1 cache. Fast access.\n",
    "2. **Step up**: Data spills to L2. Slower.\n",
    "3. **Another step**: Data spills to L3. Even slower.\n",
    "4. **Final step**: Data goes to DRAM. Much slower.\n",
    "\n",
    "The exact boundaries depend on your hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation 3: Stride Patterns\n",
    "\n",
    "What happens if we access every Nth element? This is called a **strided** access pattern.\n",
    "\n",
    "- Stride 1: Sequential (best)\n",
    "- Stride 8: Every 8th element (uses 1 of 8 per cache line)\n",
    "- Stride 64: Every 64th byte (uses 1 per cache line)\n",
    "- Stride > cache line: Maximum cache waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_strides():\n",
    "    \"\"\"Measure how access stride affects performance.\"\"\"\n",
    "    \n",
    "    # Large array to ensure we're measuring memory effects\n",
    "    n = 100_000_000\n",
    "    arr = np.arange(n, dtype=np.int64)\n",
    "    \n",
    "    strides = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "    results = []\n",
    "    \n",
    "    for stride in strides:\n",
    "        # Create indices with this stride\n",
    "        indices = np.arange(0, n, stride, dtype=np.int64)\n",
    "        num_accesses = len(indices)\n",
    "        \n",
    "        # Warmup\n",
    "        _ = arr[indices[:1000]].sum()\n",
    "        \n",
    "        # Measure\n",
    "        times = []\n",
    "        for _ in range(3):\n",
    "            start = time.perf_counter()\n",
    "            _ = arr[indices].sum()\n",
    "            times.append(time.perf_counter() - start)\n",
    "        \n",
    "        mean_time = np.mean(times)\n",
    "        gb_per_sec = (num_accesses * 8) / mean_time / 1e9\n",
    "        \n",
    "        results.append({\n",
    "            'stride': stride,\n",
    "            'gb_per_sec': gb_per_sec,\n",
    "            'time': mean_time,\n",
    "        })\n",
    "        \n",
    "        print(f\"Stride {stride:>3}: {gb_per_sec:>6.2f} GB/s effective bandwidth\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "stride_results = benchmark_strides()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stride effects\n",
    "strides = [r['stride'] for r in stride_results]\n",
    "bandwidths = [r['gb_per_sec'] for r in stride_results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(strides)), bandwidths, tick_label=strides)\n",
    "plt.xlabel('Stride (elements)', fontsize=12)\n",
    "plt.ylabel('Effective Bandwidth (GB/s)', fontsize=12)\n",
    "plt.title('How Stride Affects Memory Bandwidth', fontsize=14)\n",
    "\n",
    "# Annotate cache line boundary\n",
    "cache_line_elements = 64 // 8  # 8 bytes per int64\n",
    "plt.axvline(x=strides.index(cache_line_elements) if cache_line_elements in strides else -1, \n",
    "            color='red', linestyle='--', alpha=0.5, label='Cache line boundary')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStride 1 to Stride {strides[-1]} bandwidth ratio: {bandwidths[0]/bandwidths[-1]:.1f}×\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observation\n",
    "\n",
    "Notice how bandwidth **drops dramatically** as stride increases past the cache line size (8 elements for int64, since 64 bytes / 8 bytes per element = 8).\n",
    "\n",
    "At stride 8+, you're fetching a full cache line but only using 1 element from it. That's 87.5% waste!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn: Experiments\n",
    "\n",
    "Try modifying the benchmarks to answer these questions:\n",
    "\n",
    "1. **Different data types**: How do float32 vs float64 vs int8 affect the results?\n",
    "\n",
    "2. **Read vs write**: Is writing faster or slower than reading? (Hint: try `arr[indices] = value` vs `arr[indices].sum()`)\n",
    "\n",
    "3. **Multiple passes**: If you access the same data multiple times, does the second pass benefit from caching?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here!\n",
    "\n",
    "# Example: Try different data types\n",
    "def compare_dtypes():\n",
    "    n = 10_000_000\n",
    "    \n",
    "    for dtype in [np.float32, np.float64, np.int8, np.int32, np.int64]:\n",
    "        arr = np.arange(n, dtype=dtype)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        for _ in range(10):\n",
    "            _ = arr.sum()\n",
    "        elapsed = time.perf_counter() - start\n",
    "        \n",
    "        bytes_per_elem = arr.itemsize\n",
    "        gb_per_sec = (n * bytes_per_elem * 10) / elapsed / 1e9\n",
    "        \n",
    "        print(f\"{dtype.__name__:>10} ({bytes_per_elem} bytes): {gb_per_sec:.2f} GB/s\")\n",
    "\n",
    "compare_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **The RAM model lies**: Memory access is NOT uniform. It can vary by 200× depending on where data lives.\n",
    "\n",
    "2. **Sequential beats random**: By 5-20×, because of caching and prefetching.\n",
    "\n",
    "3. **Working set size matters**: Keep frequently-used data small enough to fit in cache.\n",
    "\n",
    "4. **Stride kills performance**: Strided access wastes cache lines. Use stride-1 (sequential) when possible.\n",
    "\n",
    "5. **Measure on YOUR hardware**: Cache sizes and latencies vary. The specific numbers depend on your machine.\n",
    "\n",
    "---\n",
    "\n",
    "*Continue to [Chapter 2: The Tyranny of Bandwidth](https://ttsugriy.github.io/performance-book/chapters/02-bandwidth-tyranny.html)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

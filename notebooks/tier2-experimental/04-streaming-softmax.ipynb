{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-0",
   "source": [
    "# Investigation: Streaming Softmax\n",
    "\n",
    "**From The Nature of Fast, Chapter 4: Chunking**\n",
    "\n",
    "This notebook derives the streaming softmax algorithm step by step. You'll discover how associativity enables computing softmax without materializing the entire attention matrix.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ttsugriy/performance-book/blob/main/notebooks/tier2-experimental/04-streaming-softmax.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-1",
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-2",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For numerical stability\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-3",
   "source": [
    "## The Problem: Softmax Needs Everything\n",
    "\n",
    "Standard softmax requires seeing all values to compute the denominator:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "This means we can't compute any output until we've seen all inputs. Or can we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-4",
   "outputs": [],
   "source": [
    "def naive_softmax(x):\n",
    "    \"\"\"Standard softmax - requires all values.\"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Test it\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Softmax: {naive_softmax(x)}\")\n",
    "print(f\"Sum (should be 1.0): {naive_softmax(x).sum():.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-5",
   "source": [
    "### Problem: Numerical Overflow\n",
    "\n",
    "What happens with large values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-6",
   "outputs": [],
   "source": [
    "# Large values cause overflow\n",
    "x_large = np.array([1000.0, 1001.0, 1002.0, 1003.0])\n",
    "print(f\"Input: {x_large}\")\n",
    "print(f\"exp(1000) = {np.exp(1000)}\")\n",
    "print(f\"Naive softmax: {naive_softmax(x_large)}\")\n",
    "print(\"\\nâ˜ ï¸ NaN! The naive approach overflows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-7",
   "source": [
    "## Step 1: The Max Trick (Stable Softmax)\n",
    "\n",
    "Subtract the maximum value before exponentiating:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}$$\n",
    "\n",
    "This is mathematically equivalent (the max cancels out) but numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-8",
   "outputs": [],
   "source": [
    "def stable_softmax(x):\n",
    "    \"\"\"Numerically stable softmax using max trick.\"\"\"\n",
    "    x_max = np.max(x)\n",
    "    exp_x = np.exp(x - x_max)  # Subtract max before exp\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Now it works!\n",
    "print(f\"Input: {x_large}\")\n",
    "print(f\"Stable softmax: {stable_softmax(x_large)}\")\n",
    "print(f\"Sum: {stable_softmax(x_large).sum():.10f}\")\n",
    "\n",
    "# Verify equivalence for normal values\n",
    "print(f\"\\nNaive and stable match: {np.allclose(naive_softmax(x), stable_softmax(x))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-9",
   "source": [
    "## Step 2: The Streaming Challenge\n",
    "\n",
    "But we still need to see all values to find the max!\n",
    "\n",
    "**Question**: Can we compute softmax in chunks, without ever holding all values in memory?\n",
    "\n",
    "Let's think about what we need:\n",
    "1. The maximum value (to prevent overflow)\n",
    "2. The sum of exponentials (for normalization)\n",
    "\n",
    "Can we update these incrementally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-10",
   "outputs": [],
   "source": [
    "# First attempt: Process in chunks, track running max and sum\n",
    "def chunked_softmax_v1(x, chunk_size=2):\n",
    "    \"\"\"First attempt at chunked softmax.\"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    # First pass: find max\n",
    "    running_max = float('-inf')\n",
    "    for i in range(0, n, chunk_size):\n",
    "        chunk = x[i:i+chunk_size]\n",
    "        chunk_max = np.max(chunk)\n",
    "        running_max = max(running_max, chunk_max)\n",
    "    \n",
    "    # Second pass: compute exp and sum\n",
    "    running_sum = 0\n",
    "    for i in range(0, n, chunk_size):\n",
    "        chunk = x[i:i+chunk_size]\n",
    "        running_sum += np.sum(np.exp(chunk - running_max))\n",
    "    \n",
    "    # Third pass: compute output\n",
    "    output = np.zeros(n)\n",
    "    for i in range(0, n, chunk_size):\n",
    "        chunk = x[i:i+chunk_size]\n",
    "        output[i:i+chunk_size] = np.exp(chunk - running_max) / running_sum\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test it\n",
    "result = chunked_softmax_v1(x)\n",
    "print(f\"Chunked v1: {result}\")\n",
    "print(f\"Matches stable: {np.allclose(result, stable_softmax(x))}\")\n",
    "print(f\"\\nâš ï¸ Problem: We needed THREE passes over the data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-11",
   "source": [
    "## Step 3: The Insight - One Pass Is Possible!\n",
    "\n",
    "Here's the key insight: when we see a new maximum, we can **rescale** our running sum.\n",
    "\n",
    "If we've computed $\\sum e^{x_i - m_{old}}$ and find a new max $m_{new}$, we can convert:\n",
    "\n",
    "$$\\sum e^{x_i - m_{old}} \\cdot e^{m_{old} - m_{new}} = \\sum e^{x_i - m_{new}}$$\n",
    "\n",
    "This lets us maintain a running sum while updating the max!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-12",
   "outputs": [],
   "source": [
    "def streaming_softmax_stats(x, chunk_size=2):\n",
    "    \"\"\"Compute softmax max and sum in ONE pass using rescaling.\"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    # State: (running_max, running_sum)\n",
    "    running_max = float('-inf')\n",
    "    running_sum = 0.0\n",
    "    \n",
    "    for i in range(0, n, chunk_size):\n",
    "        chunk = x[i:i+chunk_size]\n",
    "        chunk_max = np.max(chunk)\n",
    "        \n",
    "        if chunk_max > running_max:\n",
    "            # Rescale the running sum!\n",
    "            running_sum = running_sum * np.exp(running_max - chunk_max)\n",
    "            running_max = chunk_max\n",
    "        \n",
    "        # Add this chunk's contribution\n",
    "        running_sum += np.sum(np.exp(chunk - running_max))\n",
    "        \n",
    "        print(f\"After chunk {i//chunk_size + 1}: max={running_max:.2f}, sum={running_sum:.4f}\")\n",
    "    \n",
    "    return running_max, running_sum\n",
    "\n",
    "# Test on our example\n",
    "print(f\"Input: {x}\")\n",
    "final_max, final_sum = streaming_softmax_stats(x)\n",
    "print(f\"\\nFinal: max={final_max}, sum={final_sum}\")\n",
    "\n",
    "# Verify against standard approach\n",
    "expected_max = np.max(x)\n",
    "expected_sum = np.sum(np.exp(x - expected_max))\n",
    "print(f\"Expected: max={expected_max}, sum={expected_sum}\")\n",
    "print(f\"Match: {np.isclose(final_max, expected_max) and np.isclose(final_sum, expected_sum)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-13",
   "source": [
    "## Step 4: The Full Streaming Softmax\n",
    "\n",
    "For attention, we don't just want softmax valuesâ€”we want the **weighted sum** of values V.\n",
    "\n",
    "We need to track: (max, sum, weighted_output)\n",
    "\n",
    "When the max changes, we rescale both sum AND the accumulated output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-14",
   "outputs": [],
   "source": [
    "def streaming_attention(scores, values, chunk_size=2):\n",
    "    \"\"\"\n",
    "    Compute softmax(scores) @ values in a streaming fashion.\n",
    "    \n",
    "    This is the core of FlashAttention!\n",
    "    \n",
    "    Args:\n",
    "        scores: [n] attention scores for one query\n",
    "        values: [n, d] value vectors\n",
    "        chunk_size: how many scores/values to process at once\n",
    "    \n",
    "    Returns:\n",
    "        [d] output vector = softmax(scores) @ values\n",
    "    \"\"\"\n",
    "    n = len(scores)\n",
    "    d = values.shape[1]\n",
    "    \n",
    "    # State: (running_max, running_sum, running_output)\n",
    "    running_max = float('-inf')\n",
    "    running_sum = 0.0\n",
    "    running_output = np.zeros(d)\n",
    "    \n",
    "    for i in range(0, n, chunk_size):\n",
    "        chunk_scores = scores[i:i+chunk_size]\n",
    "        chunk_values = values[i:i+chunk_size]\n",
    "        chunk_max = np.max(chunk_scores)\n",
    "        \n",
    "        if chunk_max > running_max:\n",
    "            # Rescale everything!\n",
    "            scale = np.exp(running_max - chunk_max)\n",
    "            running_sum = running_sum * scale\n",
    "            running_output = running_output * scale\n",
    "            running_max = chunk_max\n",
    "        \n",
    "        # Compute weights for this chunk\n",
    "        chunk_weights = np.exp(chunk_scores - running_max)\n",
    "        \n",
    "        # Update running totals\n",
    "        running_sum += np.sum(chunk_weights)\n",
    "        running_output += chunk_weights @ chunk_values  # Weighted sum\n",
    "    \n",
    "    # Final normalization\n",
    "    return running_output / running_sum\n",
    "\n",
    "# Test it!\n",
    "np.random.seed(42)\n",
    "n, d = 8, 4\n",
    "scores = np.random.randn(n)\n",
    "values = np.random.randn(n, d)\n",
    "\n",
    "# Streaming version\n",
    "streaming_result = streaming_attention(scores, values, chunk_size=2)\n",
    "\n",
    "# Standard version for comparison\n",
    "weights = stable_softmax(scores)\n",
    "standard_result = weights @ values\n",
    "\n",
    "print(f\"Streaming result: {streaming_result}\")\n",
    "print(f\"Standard result:  {standard_result}\")\n",
    "print(f\"\\nMatch: {np.allclose(streaming_result, standard_result)}\")\n",
    "print(f\"Max difference: {np.max(np.abs(streaming_result - standard_result)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-15",
   "source": [
    "## ðŸŽ‰ We Did It!\n",
    "\n",
    "We just derived the core algorithm of FlashAttention:\n",
    "\n",
    "1. Process scores and values in chunks\n",
    "2. Maintain running (max, sum, output) state\n",
    "3. Rescale when max changes\n",
    "4. Normalize at the end\n",
    "\n",
    "This means we never need to store the full NÃ—N attention matrix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-16",
   "source": [
    "## Investigation: Memory Savings\n",
    "\n",
    "Let's quantify how much memory this saves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-17",
   "outputs": [],
   "source": [
    "def memory_comparison(seq_lengths):\n",
    "    \"\"\"Compare memory usage: standard vs streaming.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for n in seq_lengths:\n",
    "        # Standard attention: stores NÃ—N matrix\n",
    "        standard_memory = n * n * 4  # float32 = 4 bytes\n",
    "        \n",
    "        # Streaming: only stores chunk_size scores + running state\n",
    "        chunk_size = 64  # Typical FlashAttention block size\n",
    "        streaming_memory = chunk_size * 4 + 3 * 4  # chunk + (max, sum, one output float)\n",
    "        \n",
    "        results.append({\n",
    "            'n': n,\n",
    "            'standard_mb': standard_memory / 1e6,\n",
    "            'streaming_mb': streaming_memory / 1e6,\n",
    "            'ratio': standard_memory / streaming_memory,\n",
    "        })\n",
    "        \n",
    "        print(f\"N={n:>6}: Standard={standard_memory/1e6:>8.2f} MB, \"\n",
    "              f\"Streaming={streaming_memory/1e6:>8.4f} MB, \"\n",
    "              f\"Savings={standard_memory/streaming_memory:>8.0f}Ã—\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]\n",
    "results = memory_comparison(seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-18",
   "outputs": [],
   "source": [
    "# Visualize\n",
    "ns = [r['n'] for r in results]\n",
    "standard = [r['standard_mb'] for r in results]\n",
    "streaming = [r['streaming_mb'] for r in results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(ns, standard, 'o-', label='Standard (NÂ² memory)', linewidth=2, markersize=8)\n",
    "plt.loglog(ns, streaming, 's-', label='Streaming (constant memory)', linewidth=2, markersize=8)\n",
    "\n",
    "# Add GPU memory limit\n",
    "plt.axhline(y=16000, color='red', linestyle='--', alpha=0.5, label='16 GB GPU memory')\n",
    "\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Memory (MB)', fontsize=12)\n",
    "plt.title('Attention Memory: Standard vs Streaming', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Insight: Standard attention runs out of memory around N=65K.\")\n",
    "print(\"   Streaming can handle millions of tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-19",
   "source": [
    "## The Associativity Connection\n",
    "\n",
    "Why does this work? Because of **associativity**.\n",
    "\n",
    "The key operation is combining two (max, sum, output) tuples:\n",
    "\n",
    "```\n",
    "combine((m1, s1, o1), (m2, s2, o2)) = (\n",
    "    max(m1, m2),\n",
    "    s1 * exp(m1 - new_max) + s2 * exp(m2 - new_max),\n",
    "    o1 * exp(m1 - new_max) + o2 * exp(m2 - new_max)\n",
    ")\n",
    "```\n",
    "\n",
    "This combination is **associative**: `combine(combine(a, b), c) = combine(a, combine(b, c))`\n",
    "\n",
    "Which means we can chunk arbitrarily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-20",
   "outputs": [],
   "source": [
    "def combine_states(state1, state2):\n",
    "    \"\"\"Combine two (max, sum, output) states.\"\"\"\n",
    "    m1, s1, o1 = state1\n",
    "    m2, s2, o2 = state2\n",
    "    \n",
    "    new_max = max(m1, m2)\n",
    "    scale1 = np.exp(m1 - new_max)\n",
    "    scale2 = np.exp(m2 - new_max)\n",
    "    \n",
    "    new_sum = s1 * scale1 + s2 * scale2\n",
    "    new_output = o1 * scale1 + o2 * scale2\n",
    "    \n",
    "    return (new_max, new_sum, new_output)\n",
    "\n",
    "# Verify associativity\n",
    "np.random.seed(42)\n",
    "a = (np.random.randn(), np.random.rand() + 0.1, np.random.randn())\n",
    "b = (np.random.randn(), np.random.rand() + 0.1, np.random.randn())\n",
    "c = (np.random.randn(), np.random.rand() + 0.1, np.random.randn())\n",
    "\n",
    "# (a âŠ• b) âŠ• c\n",
    "left = combine_states(combine_states(a, b), c)\n",
    "\n",
    "# a âŠ• (b âŠ• c)\n",
    "right = combine_states(a, combine_states(b, c))\n",
    "\n",
    "print(f\"(a âŠ• b) âŠ• c = {left}\")\n",
    "print(f\"a âŠ• (b âŠ• c) = {right}\")\n",
    "print(f\"\\nAssociative: {np.allclose(left, right)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-21",
   "source": [
    "## Your Turn: Experiments\n",
    "\n",
    "1. **Different chunk sizes**: How does chunk size affect numerical accuracy?\n",
    "\n",
    "2. **Parallel reduction**: Since `combine` is associative, we can parallelize! Implement a parallel version.\n",
    "\n",
    "3. **Multi-query**: Extend to handle multiple queries (multiple rows of Q)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-22",
   "outputs": [],
   "source": [
    "# Your experiments here!\n",
    "\n",
    "# Example: Test different chunk sizes\n",
    "def test_chunk_sizes():\n",
    "    np.random.seed(123)\n",
    "    n, d = 1024, 64\n",
    "    scores = np.random.randn(n) * 2  # Larger variance for more interesting behavior\n",
    "    values = np.random.randn(n, d)\n",
    "    \n",
    "    # Ground truth\n",
    "    weights = stable_softmax(scores)\n",
    "    ground_truth = weights @ values\n",
    "    \n",
    "    for chunk_size in [1, 2, 4, 8, 16, 32, 64, 128, 256]:\n",
    "        result = streaming_attention(scores, values, chunk_size)\n",
    "        max_error = np.max(np.abs(result - ground_truth))\n",
    "        print(f\"Chunk size {chunk_size:>4}: max error = {max_error:.2e}\")\n",
    "\n",
    "test_chunk_sizes()\n",
    "print(\"\\nðŸ’¡ Notice: All chunk sizes give the same answer (within floating point precision)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-23",
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **The max trick** prevents overflow by subtracting max before exp.\n",
    "\n",
    "2. **Rescaling** lets us update a running sum when we find a new max.\n",
    "\n",
    "3. **Streaming attention** maintains (max, sum, output) state and updates incrementally.\n",
    "\n",
    "4. **Associativity** is what makes chunking possibleâ€”we can combine partial results in any order.\n",
    "\n",
    "5. **Memory savings** are dramatic: O(NÂ²) â†’ O(1) for the attention matrix.\n",
    "\n",
    "This is the mathematical foundation of FlashAttention!\n",
    "\n",
    "---\n",
    "\n",
    "*Continue to [Chapter 11: FlashAttention Investigation](https://ttsugriy.github.io/performance-book/chapters/11-flash-attention.html)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

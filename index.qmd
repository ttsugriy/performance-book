---
title: "The Algebra of Speed"
subtitle: "Mathematical Foundations of Computational Performance"
---

# Preface {.unnumbered}

FlashAttention delivers 2-4√ó speedups. LoRA fine-tunes 65-billion-parameter models on a single GPU. Mixtral matches models 5√ó its compute budget.

Ask practitioners *why* these techniques work, and you get implementation details: tiling, low-rank adapters, routing functions.

But those are *how*, not *why*.

The *why* is mathematical. FlashAttention works because softmax has associative structure‚Äîa property that licenses chunking. LoRA works because fine-tuning is low-rank‚Äîa property that licenses factorization. Mixtral works because different inputs need different parameters‚Äîa property that licenses conditional computation.

**Properties explain optimizations.**

This book is about those properties. About recognizing them. About knowing when to apply them. About developing the problem-solver's eye that sees not "here's a trick that worked" but "here's a structure that enables a class of tricks."

---

## What This Book Is

This is not a recipe book. It won't tell you "to optimize X, do Y."

This is a book about **understanding**. Each chapter is an *investigation*:

1. We start with something puzzling‚Äîa phenomenon that demands explanation
2. We form hypotheses and test them
3. We're sometimes wrong‚Äîand the wrongness is instructive
4. We reach understanding and extract something general

The structure mirrors how performance understanding actually develops. It's messy. It's iterative. The answer isn't obvious from the start. You develop intuition by being wrong and correcting.

---

## The Three Pillars

Performance lives at the intersection of three domains:

```
          MATHEMATICS
              ‚îÇ
              ‚îÇ What structures make
              ‚îÇ computation tractable?
              ‚îÇ
              ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                     ‚îÇ
    ‚îÇ   PERFORMANCE       ‚îÇ
    ‚îÇ                     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚ñ≤
             ‚ï± ‚ï≤
            ‚ï±   ‚ï≤
           ‚ï±     ‚ï≤
          ‚ï±       ‚ï≤
   HARDWARE        METHODOLOGY
       ‚îÇ                ‚îÇ
       ‚îÇ                ‚îÇ
  How does the     How do we find
  machine reward   the right
  structure?       structure?
```

**Mathematics** provides the properties: associativity, locality, separability, sparsity. These determine what transformations are legal.

**Hardware** provides the constraints: memory hierarchies, parallelism, bandwidth limits. These determine what transformations are profitable.

**Methodology** provides the process: measurement, hypothesis, analogy, verification. This is how we discover which properties apply to our problem.

Most performance resources cover one pillar. This book weaves all three together, because understanding requires seeing across levels.

---

## Who This Book Is For

**Primary audience**: Engineers and researchers who work on performance-critical code, especially ML systems.

**Assumed background**:

- Comfortable with code (Python, C/C++, or similar)
- Basic understanding of computer architecture (caches, cores, memory)
- Some math (linear algebra, basic calculus)
- Curiosity about *why* things work, not just *what* works

**Not for**:

- Complete beginners (need programming foundations first)
- Readers seeking quick tips without understanding
- Those who just want to copy-paste optimizations

---

## How to Read This Book

The book is designed for multiple reading patterns:

**Linear reading**: Parts build on each other. The Algebraic Framework establishes the theory. Part I covers hardware. Part II introduces properties. Parts III-IV apply them to algorithms and systems. Part V covers methodology, Part VI provides practical tools, and Part VII synthesizes.

**Investigation hopping**: Each investigation in Parts III-IV is somewhat self-contained. If you're curious about FlashAttention specifically, you can start there‚Äîwith occasional references back to earlier material.

**Interactive exploration**: Many chapters include embedded visualizations and linked notebooks. The investigations come alive when you *do* them, not just read them.

---

## Learning Paths

Different readers have different goals. Here are recommended paths through the material:

### Path A: "I want to understand the theory"

For researchers and those seeking deep understanding of *why* optimizations work.

```
The Algebraic Framework ‚Üí Part I (Hardware) ‚Üí Part II (Algebra)
    ‚Üí FlashAttention ‚Üí LoRA ‚Üí State Space Models
```

**Focus on:** Mathematical derivations, property recognition, first-principles reasoning.

**Skip:** Hardware reference appendix, tool-specific chapters (can revisit later).

### Path B: "I want to optimize my ML system"

For practitioners building production systems who need practical speedups.

```
The Algebraic Framework (skim) ‚Üí Memory Hierarchy ‚Üí GPU Architecture
    ‚Üí Inference ‚Üí Advanced Serving ‚Üí GPU Memory ‚Üí Quantization
    ‚Üí Profiling Tools ‚Üí torch.compile
```

**Focus on:** Bottleneck identification, configuration tuning, practical patterns.

**Skip:** Mathematical derivations (can revisit for deeper understanding).

### Path C: "I want to write custom kernels"

For engineers who need to implement novel operations or optimize beyond libraries.

```
GPU Architecture ‚Üí Matrix Multiply ‚Üí FlashAttention
    ‚Üí Triton ‚Üí GPU Kernel Frameworks ‚Üí Profiling Tools
```

**Focus on:** Memory hierarchy on GPU, tiling patterns, Triton/CUDA programming.

**Skip:** High-level systems chapters (serving, distributed) initially.

### Path D: "I want to understand a specific topic"

Each major topic is approachable with minimal prerequisites:

| Topic | Prerequisites | Chapter(s) |
|-------|---------------|------------|
| FlashAttention | Algebraic Framework, Memory Hierarchy | FlashAttention |
| LoRA | Algebraic Framework, Factoring | LoRA |
| Quantization | Memory Hierarchy, Bandwidth | Quantization |
| Distributed Training | Parallelism, GPU Architecture | Distributed |
| LLM Serving | Inference, GPU Memory | Advanced Serving |
| MoE | Skipping, Distributed | Mixture of Experts |

---

## The Interactive Elements

This book has three layers of interactivity:

### Embedded Visualizations

Interactive diagrams run directly in your browser. Adjust parameters, see effects. No installation required.

### Quick Experiments (JupyterLite)

Python notebooks that run entirely in your browser via WebAssembly. Good for understanding algorithms. Not suitable for performance measurement (WASM is slow).

### GPU Experiments (Colab/Kaggle)

Real notebooks with real GPUs. This is where you measure actual speedups. Free tier is sufficient for all examples.

::: {.callout-tip}
## A Note on Performance Numbers

Cloud notebooks have variable performance. Your results may differ from the book's.

Focus on **relative** speedups (2√ó faster) rather than **absolute** times (23ms). Relative speedups are more stable across hardware.

For serious benchmarking, run locally on controlled hardware.
:::

---

## The Thesis

If there's one idea this book wants to convey, it's this:

> **The algebra isn't abstract. It's why modern machine learning is computationally tractable at all.**

FlashAttention isn't magic‚Äîit's associativity. LoRA isn't magic‚Äîit's separability. Quantization isn't magic‚Äîit's redundancy.

Once you see the mathematical structure, you can derive the technique. And you can recognize when the same structure appears in a new problem, waiting to be exploited.

That's the skill this book aims to develop: seeing the properties that enable performance, not just memorizing the tricks that result from them.

---

## Acknowledgments

This book stands on the shoulders of giants:

- **Alexander Stepanov** and Daniel Rose, whose *From Mathematics to Generic Programming* showed that abstract algebra explains practical programming
- **Brendan Gregg**, whose methodologies brought rigor to systems performance
- **George Polya**, whose *How to Solve It* taught generations how to think about problems
- The **explorable explanations** community, for showing that interactivity enhances understanding
- The countless researchers whose work this book attempts to explain and connect

---

## Let's Begin

Before diving into hardware specifics or optimization techniques, we need to establish the right mental model.

The next chapter introduces **Thinking in Arrays**‚Äîthe cognitive shift from loop-based programming to array-oriented computation. This isn't just syntax; it's a fundamentally different way of seeing programs that makes mathematical structure visible.

Following that, **The Algebraic Framework** provides the vocabulary‚Äîthe six fundamental properties that enable all significant optimizations. Together, these two chapters form the conceptual foundation for everything that follows.

Let's begin.

[**Thinking in Arrays ‚Üí**](chapters/00a-thinking-in-arrays.qmd)

---

## More from the Author

### The First Principles Trilogy

This book is part of a series teaching ML fundamentals from first principles:

üìò **[Building LLMs from First Principles](https://ttsugriy.github.io/llm-first-principles/)**
Learn how transformers work by building them from scratch‚Äîfull math derivations, working code, and comprehensive test suites. From Markov chains to GPT.

üî¨ **[Mechanistic Interpretability from First Principles](https://ttsugriy.github.io/mechinterp-first-principles/)**
Reverse-engineer neural networks to understand their internal algorithms. Features, superposition, circuits, and sparse autoencoders explained from the ground up.

‚ö° **The Algebra of Speed** *(You are here)*
Mathematical foundations of computational performance. Why FlashAttention, LoRA, and quantization work‚Äîand how to recognize when similar optimizations apply to your problems.

üåê **[Distributed Training from First Principles](https://ttsugriy.github.io/distributed-training-book/)**
Deep dive into distributed training‚Äîdata parallelism, tensor parallelism, pipeline parallelism, and beyond. The natural continuation of this book's distributed chapter.

### Blog

‚úçÔ∏è **[Software Bits](https://softwarebits.substack.com/)** ‚Äî Short, focused essays on performance, ML, and computer science fundamentals. Subscribe for updates.

üíª **[GitHub: perf-bits](https://github.com/ttsugriy/perf-bits)** ‚Äî Blog posts with full code and interactive demos.
